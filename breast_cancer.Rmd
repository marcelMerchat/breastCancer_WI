---
title: "Breast Cancer Prediction with Measurements of Tumor Cell Nuclei"

output: 
  html_document
    
---

<br><br>

##      Prediction and Detection of Malignant Nuclei from Breast Cancer Tumors

#####    February 1, 2017
Prepared by Marcel Merchat

<br><br>

##     Overview
We explore ten cell dimension variables and a corresponding set of worst dimensions derived from fine needle aspiration (FNA) samples of breast tumors. The data is taken from the University of Wisconsin dataset that originated with Dr. William Wolberg. The analysis quantifies how the worst dimensions of malignant nuclei are greater than the worst dimensions of healthier benign ones. Finally, we develop an algorithm that predicts whether tumors are malignant or benign from a training data set and evaluate it using a test data set. 

We adjust the algorithm to increase the probability of detecting malignant tumors which raises the rate of false alarms. The relationship between these probabilites are illustrated by the characteristic curve in Figure-7. In statistical detection theory, this plot is called a receiver operating curve (ROC). 

The measurements were taken with a digital camera and a computer software program called Xcyt. A technician isolates individual nuclei and draws the approximate boundary of each nucleus using a computer mouse pointer in an technique known as "snakes." This is repeated for at least 10 cell nuclei for each sample and the software program smooths the pointer data and calculates average nuclear boundaries. A similar process is performed for each of the ten nuclei characteristics, measuring size, shape, and texture. Finally, the mean, standard error and extreme values of these characteristics are computed. 

<br><br>

##    Raw Data

There are a total of 569 observation records consisting of 357 benign (B) and 212 malignant (M) cases. The data file is available from the University of Wisconsin server at http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/WDBC.dat. Since this source lacked column names, the names were taken from an updated version that could only be manually downloaded at https://www.kaggle.com/uciml/breast-cancer-wisconsin-data. There are 32 columns of data including identification, diagnosis, and three groups of ten variables. The statistical mean values for the ten variables below are reported in Columns 3-12. 

#### Information:

https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names and http://pages.cs.wisc.edu/~olvi/uwmp/cancer.html

Study Paper by William Wolberg, Nick Street, Dennis Heisey, and Olvi L. Mangasarian https://dollar.biz.uiowa.edu/~street/research/hu_path95/hp95.pdf


#### Observations: 

###### a) radius (mean of distances from center to points on the perimeter) 
###### b) texture (standard deviation of gray-scale values)
###### c) perimeter
###### d) area
###### e) smoothness (local variation in radius lengths)
###### f) compactness (perimeter^2 / area - 1.0)
###### g) concavity (severity of concave portions of the contour)
###### h) concave points (number of concave portions of the contour)
###### i) symmetry
###### j) fractal dimension ("coastline approximation" - 1)

The corresponding standard errors for the ten variables are reported in Columns 13-22 and the correspsonding worst values in Columns 23-32. For example, since Field-3 is the mean radius, Field-13 is standard error (se) of the radius and Field-23 is the worst radius. 

<br><br>

##   Selection of Training Set

The data was divided into model building and validation sets. The build set was then further divided using a two-fold partition for training and model algorithm tryout testing.

<br><br>

##   Exploration

```{r setup, results='hide', echo = FALSE, message=F, warning=F}

##library(dplyr)
library(lattice)

library(ggplot2)
library(caret)
library(glmnet)
library(randomForest)
library(AppliedPredictiveModeling)

library(psych)
library(xtable)


library(grid)
library(gridExtra)
library(stats)

library(pROC)
library(plotROC)

oldw <- getOption("warn")
options(warn = -1)

```


```{r raw_data, echo=FALSE, results='hide'}

fileurl <- "http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/WDBC.dat"
## fileurl <-  "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29" metadata
##download.file(fileurl, "breast_cancer_wi_data.csv")

raw_data <- read.csv("~/GitHub/Cancer/breast_cancer_wi_data.csv", header = FALSE, check.names = FALSE)
col_names <-  read.csv("~/GitHub/Cancer/raw/column_names.csv", header = FALSE, nrows = 1)
#colnames(raw_data) <- unname(t(col_names)[,1])
colnames(raw_data) <- c("id","diagnosis",
                "radius_mean", "texture_mean",
                "perimeter_mean", "area_mean",
                "smoothness_mean", "compactness_mean",
                "concavity_mean","concave_points_mean",
                "symmetry_mean", "fractal_dimension_mean",
               
                "radius_se", "texture_se",
                "perimeter_se", "area_se",
                "smoothness_se", "compactness_se",
                "concavity_se", "concave_points_se",
                "symmetry_se", "fractal_dimension_se",
                 
                "radius_worst", "texture_worst",
                "perimeter_worst", "area_worst",
                "smoothness_worst", "compactness_worst",
                "concavity_worst", "concave_points_worst",
                "symmetry_worst", "fractal_dimension_worst"
               )

# test1 <- c(1:5, "6,7", "8,9,10")
# tf <- tempfile()
# writeLines(test1, "~/GitHub/Cancer/tf")
# 
# rd <- read.csv("~/GitHub/Cancer/tf", fill = TRUE) # 1 column

test_names <- c(colnames(raw_data)[3:12],colnames(raw_data)[23:32])

rearranged_data <- raw_data[,-2] ## remove diagnosis from second column
diagnosis_raw <- raw_data[,2]  #as.factor(raw_data[,2])

rearranged_data[,"diagnosis"] <- diagnosis_raw 
id <- rearranged_data[,"id"]
id <- sub("^ +", "", id)
id <- sub(" +$", "", id)
id_length <- nchar(as.character(rearranged_data[,"id"]))

rearranged_data[,"Group_ID"] <- as.factor(id_length)

specialgroup <- rearranged_data[rearranged_data$Group_ID=="8",]
dim(specialgroup[specialgroup$diagnosis=="B",]) #  37
dim(specialgroup[specialgroup$diagnosis=="M",]) #  33

set.seed(3433)
inBuild <- createDataPartition(y=rearranged_data$diagnosis,p=0.7,list=FALSE)
build <- rearranged_data[inBuild,]

build[,"perimeter_concavity_product"] <- build[,"perimeter_mean"]  * build[,"concave_points_worst"] ##########################
validation <- rearranged_data[-inBuild,]

set.seed(32323)
folds <- createFolds(y=build$diagnosis,k=3,list=TRUE,returnTrain=FALSE)
#sapply(folds,length)

testing <- build[folds$Fold1,]
training  <- rbind(build[folds$Fold2,],build[folds$Fold3,])

train4 <- training[training$Group_ID=="4",]
train5 <- training[training$Group_ID=="5",]
train6 <- training[training$Group_ID=="6",]
train7 <- training[training$Group_ID=="7",]
train8 <- training[training$Group_ID=="8",]

test4  <-  testing[ testing$Group_ID=="4",]
test5  <-  testing[ testing$Group_ID=="5",]
test6  <-  testing[ testing$Group_ID=="6",]
test7  <-  testing[ testing$Group_ID=="7",]

test8  <-  testing[ testing$Group_ID=="8",]
train9 <- training[training$Group_ID=="9",]

train49 <- rbind(train4,train9)
rows49 <- dim(train49)[1]
train49[,"Group_ID"] <- as.factor(rep("49",rows49))

train498 <- rbind(train49,train8)
train5678 <- rbind(train5,train6,train7,train8)
test5678 <- rbind(test5,test6,test7,test8)

train78 <- rbind(train7,train8)
test78 <- rbind(test7,test8)

traina <- train78 ## train8
test_data <- test78 ### test8

```


```{r functionsection, echo=FALSE}

get_concave_points_fit <- function(df) {
        fit <- lm(df$concave_points_worst ~ df$compactness_mean)
        fit$coefficients
}

get_concave_points_2Group_plot <- function(df,m1,int1,m2,int2){
    ggplot(df, aes(compactness_mean, concave_points_worst)) + 
    geom_point(aes(color = Group_ID), alpha=1, size=3) +  
    scale_color_manual(values=c("#006633","#CCBBBB")) +   
    # xlim(0, 0.3) +
    # ylim(0, 0.3) +
    coord_cartesian(xlim = c(0, 0.3), ylim = c(0, 0.3)) +
    xlab("Mean Compactness") +
    scale_x_continuous(minor_breaks = seq(-0.1 , 1.1, 0.01), breaks = seq(-0.1, 1.1, 0.05)) +
    scale_y_continuous(minor_breaks = seq(-0.1 , 1.1, 0.01), breaks = seq(-0.1, 1.1, 0.05)) +
    geom_abline(slope = m1, intercept = int1, size=1.3, color="#006633") +
    ylab("Worst Concave Points") +
    geom_abline(slope = m2, intercept = int2, size=1.3, color="#BBAAAA") +
    ggtitle("Steeper Curve for Group 49") +
    theme(plot.title = element_text(hjust = 0.5, color="#993333", size=18, face="bold.italic"),
    axis.title.x = element_text(color="#993333", size=16),
    axis.title.y = element_text(color="#993333", size=16),
    panel.grid.minor =   element_line(colour = "white",size=0.6),
    panel.grid.major =   element_line(colour = "light blue",size=0.8),
    panel.background = element_rect(fill = "#EFE5E5", colour = "brown",
    size = 0.5, linetype = "solid"),
    plot.background = element_rect(fill = "#E0DADA"))
}

get_concave_points_plot <- function(df,m1,int1,m2,int2,titlemain){
    ggplot(df, aes(compactness_mean, concave_points_worst)) + 
    geom_point(aes(color = Group_ID), alpha=1, size=3) +  
    scale_color_manual(values=c("#00EEDD","#00CCAA","#993333","#BB3333")) +  
    xlab("Mean Compactness") +
    # xlim(0, 0.3) +
    # ylim(0, 0.3) +
    coord_cartesian(xlim = c(0, 0.3), ylim = c(0, 0.3)) +
    scale_x_continuous(minor_breaks = seq(0 , 1, 0.01), breaks = seq(0, 1, 0.05)) +
    scale_y_continuous(minor_breaks = seq(0 , 1, 0.01), breaks = seq(0, 1, 0.05)) +
    geom_abline(slope = m1, intercept = int1, size=1.3, color="#006633") +
    ylab("Worst Concave Points") +
    geom_abline(slope = m2, intercept = int2, size=1.3, color="#BBAAAA") +
    ggtitle(titlemain) +
    theme(plot.title = element_text(hjust = 0.5, color="#993333", size=18, face="bold.italic"),
    axis.title.x = element_text(color="#993333", size=16),
    axis.title.y = element_text(color="#993333", size=16),
    panel.grid.minor =   element_line(colour = "white",size=0.6),
    panel.grid.major =   element_line(colour = "light blue",size=0.8),
    panel.background = element_rect(fill = "#EFE5E5", colour = "brown",
    size = 0.5, linetype = "solid"),
    plot.background = element_rect(fill = "#E0DADA"))
}

#get_calculated_data(df, kmeansObj, m=1, b=1)
get_calculated_data <- function(df, kmeansObj, m, b, cutoff){
## Add cluster group to raw data
    concave_points_cluster <- kmeansObj$cluster                 # Level 1 is malignant
    concave_points_cluster[concave_points_cluster==1] <- 3 #"M" 
    concave_points_cluster[concave_points_cluster==2] <- 2 #"B" 
    df[,"concave_points_cluster"] <- as.factor(concave_points_cluster) 
    
## Add cluster group to raw data
    logistic_cluster <- df$perimeter_mean < 100 & df$concavity_worst <  m * df$perimeter_mean + b  + cutoff
    logistic_cluster[logistic_cluster==FALSE] <- 3 #"M" 
    logistic_cluster[logistic_cluster==TRUE] <- 2  # "B" 
    df[,"concavity_cluster"] <- as.factor(logistic_cluster) 
    
    df
}

get_adjusted_concave_points_fit <- function(df, diagnosis) {
        fit <- lm(df$concave_points_worst[df$diagnosis==diagnosis] ~ df$compactness_mean[df$diagnosis==diagnosis])
        fit$coefficients
}

get_perimeter_fit <- function(df, diagnosis) {
        fit <- lm(df$perimeter_worst[df$diagnosis==diagnosis] ~ df$perimeter_mean[df$diagnosis==diagnosis])
        fit$coefficients
}

get_concave_points_kmeans_plot <- function(df, m1, int1,m2, int2, kmeansObj, titlemain){
    ggplot(df, aes(compactness_mean, concave_points_worst)) + 
    geom_point(aes(color = diagnosis), size=3) + 
    scale_color_manual(values = c("turquoise","brown")) +
    scale_x_continuous(minor_breaks = seq(0, 0.3, 0.01), breaks = seq(0, 0.3, 0.05)) +
    scale_y_continuous(minor_breaks = seq(0, 0.3, 0.01),   breaks = seq(0, 1, 0.05)) +
    xlab("Mean Compactness") +
    coord_cartesian(xlim = c(0, 0.3), ylim = c(0, 0.3)) +
    geom_abline(slope = m1, intercept = int1, size=1.3, color="brown") +
    ylab("Worst Concave Points") +
    geom_abline(slope = m2, intercept = int2, size=1.3, color="turquoise") +
    geom_segment(aes(x = kmeansObj$centers[2,1]-0.012, y = kmeansObj$centers[2,2],
                   xend = kmeansObj$centers[2,1]+0.012, yend = kmeansObj$centers[2,2]), colour = "black") +
     geom_segment(aes(x = kmeansObj$centers[2,1], y = kmeansObj$centers[2,2]-0.015,
                   xend = kmeansObj$centers[2,1], yend = kmeansObj$centers[2,2]+0.015), colour = "black") +
     geom_segment(aes(x = kmeansObj$centers[1,1]-0.012, y = kmeansObj$centers[1,2],
                   xend = kmeansObj$centers[1,1]+0.012, yend = kmeansObj$centers[1,2]), colour = "black") +
     geom_segment(aes(x = kmeansObj$centers[1,1], y = kmeansObj$centers[1,2]-.015,
                   xend = kmeansObj$centers[1,1], yend = kmeansObj$centers[1,2]+0.015), colour = "black") +
    ggtitle(titlemain) +
    theme(plot.title = element_text(hjust = 0.5, color="#993333", size=18, face="bold.italic"),
         axis.title.x = element_text(color="#993333", size=16),
         axis.title.y = element_text(color="#993333", size=16),
         panel.grid.minor =   element_line(colour = "white",size=0.6),
         panel.grid.major =   element_line(colour = "light blue",size=0.8),
         panel.background = element_rect(fill = "#EFE5E5", colour = "brown",
             size = 0.5, linetype = "solid"),
         plot.background = element_rect(fill = "#E0DADA"))
}

get_concave_points_plot2 <- function(df, m1, int1,m2, int2, kmeansObj, titlemain="title"){
    ggplot(df, aes(perimeter_mean, concave_points_worst)) + 
    geom_point(aes(color = diagnosis), size=3) + 
    scale_color_manual(values = c("turquoise","brown")) +
    scale_x_continuous(minor_breaks = seq(0 , 200, 10), breaks = seq(0, 200, 20)) +
    scale_y_continuous(minor_breaks = seq(0 , 0.3, 0.01), breaks = seq(0, 1, 0.05)) +
    coord_cartesian(xlim = c(40, 160), ylim = c(0, 0.3)) +
    geom_abline(slope = m1, intercept = int1, size=1.3, color="brown") +
    xlab("Mean Perimeter (µm)") +
    ylab("Worst Concave Points") +
    #geom_abline(slope = m2, intercept = int2, size=1.3, color="turquoise") +
    geom_segment(aes(x = kmeansObj$centers[2,1]-4, y = kmeansObj$centers[2,2],
                   xend = kmeansObj$centers[2,1]+4, yend = kmeansObj$centers[2,2]), colour = "black") +
     geom_segment(aes(x = kmeansObj$centers[2,1], y = kmeansObj$centers[2,2]-0.015,
                   xend = kmeansObj$centers[2,1], yend = kmeansObj$centers[2,2]+0.015), colour = "black") +
     geom_segment(aes(x = kmeansObj$centers[1,1]-4, y = kmeansObj$centers[1,2],
                   xend = kmeansObj$centers[1,1]+4, yend = kmeansObj$centers[1,2]), colour = "black") +
     geom_segment(aes(x = kmeansObj$centers[1,1], y = kmeansObj$centers[1,2]-.015,
                   xend = kmeansObj$centers[1,1], yend = kmeansObj$centers[1,2]+0.015), colour = "black") +
    ggtitle(titlemain) +
    theme(plot.title = element_text(hjust = 0.5, color="#993333", size=18, face="bold.italic"),
         axis.title.x = element_text(color="#993333", size=16),
         axis.title.y = element_text(color="#993333", size=16),
         panel.grid.minor =   element_line(colour = "white",size=0.6),
         panel.grid.major =   element_line(colour = "light blue",size=0.8),
         panel.background = element_rect(fill = "#EFE5E5", colour = "brown",
             size = 0.5, linetype = "solid"),
         plot.background = element_rect(fill = "#E0DADA"))
}


get_line_parameters <- function(df,diagnosis){
    slope <- get_adjusted_concave_points_fit(df,diagnosis)[2]
    intercept <- get_adjusted_concave_points_fit(df,diagnosis)[1]
    c(slope, intercept)
}

get_perimeter_line <- function(df,diagnosis){
    fit <- get_perimeter_fit(df,diagnosis)
    slope <- fit[2]
    intercept <- fit[1]
    c(slope, intercept)
}

get_kmeans <- function(x,y){
    df <- data.frame(x,y)
    kmeans(df, centers = 2)
}

get_concavity_fit <- function(df, diagnosis) {
         fit <- lm(df$concavity_worst[df$diagnosis==diagnosis] ~ df$perimeter_mean[df$diagnosis==diagnosis])
         fit$coefficients
}

get_concavity_worst_plot <- function(df,m1,int1,titlemain){
    ggplot(df, aes(perimeter_mean, concavity_worst)) + 
    geom_point(aes(color = diagnosis), size=3) + 
    scale_color_manual(values=c("turquoise","brown")) +
    xlab("Mean Perimeter (µm)") +
    ylab("Worst Concavity") +
    scale_x_continuous(minor_breaks = seq(0 , 200, 10), breaks = seq(0, 200, 20)) +
    scale_y_continuous(minor_breaks = seq(0 , 1, 0.05), breaks = seq(0, 1, 0.2)) +
    coord_cartesian(xlim = c(40, 160), ylim = c(0, 1)) +
    geom_segment(aes(x = 40, y = 60 * m1 + int1 + 0.05,
                  xend = 100, yend = 100 * m1 + int1 + 0.05), colour = "brown", linetype=2) +
    geom_segment(aes(x = 100, y = 0,
                  xend = 100, yend = 100 * m1 + int1 + 0.05), colour = "brown", linetype=2) +
    ggtitle(titlemain) +
    theme(plot.title = element_text(hjust = 0.5, color="#993333", size=18, face="bold.italic"),
        axis.title.x = element_text(color="#993333", size=16),
        axis.title.y = element_text(color="#993333", size=16),
        panel.grid.minor =   element_line(colour = "white",size=0.6),
        panel.grid.major =   element_line(colour = "light blue",size=0.8),
        panel.background = element_rect(fill = "#EFE5E5", colour = "brown",
            size = 0.5, linetype = "solid"),
        plot.background = element_rect(fill = "#E0DADA"))
}

get_perimeter_plot <- function(df,m1,int1,titlemain){
    ggplot(df, aes(perimeter_mean, perimeter_worst)) + 
    geom_point(aes(color = diagnosis), size=3) + 
    scale_color_manual(values=c("turquoise","brown")) +
    scale_x_continuous(minor_breaks = seq(0 , 240, 10), breaks = seq(0, 240, 20)) +
    scale_y_continuous(minor_breaks = seq(0 , 240, 10), breaks = seq(0, 240, 20)) +
    coord_cartesian(xlim = c(40, 160), ylim = c(60, 220)) +
    ylab("Worst Perimeter (µm)") +
    xlab("Mean Perimeter (µm)") +
    geom_abline(slope = m1, intercept = int1, size=1.3, color="turquoise") +
    # geom_segment(aes(x = 60, y = 60 * m1 + int1 + 0.05,
    #               xend = 100, yend = 100 * m1 + int1 + 0.05), colour = "turquoise") +
    # geom_segment(aes(x = 100, y = 0,
    #               xend = 100, yend = 100 * m1 + int1 + 0.05), colour = "turquoise") +
    ggtitle(titlemain) +
    theme(plot.title = element_text(hjust = 0.5, color="#993333", size=18, face="bold.italic"),
        axis.title.x = element_text(color="#993333", size=16),
        axis.title.y = element_text(color="#993333", size=16),
        panel.grid.minor =   element_line(colour = "white",size=0.6),
        panel.grid.major =   element_line(colour = "light blue",size=0.8),
        panel.background = element_rect(fill = "#EFE5E5", colour = "brown",
            size = 0.5, linetype = "solid"),
        plot.background = element_rect(fill = "#E0DADA"))
}

one_variable_mod <- function(data_vector, diagnosis){
    dat <- data.frame(data_vector, diagnosis, stringsAsFactors = FALSE)
    colnames(dat) <- c("test_results", "diagnosis")
    head(dat)
    train(diagnosis ~ test_results, method="rf", data=dat)
}

get_sensitivity <- function(predictions, diagnosis){
    detection <- predictions == diagnosis
    df <- data.frame(predictions,diagnosis,detection,stringsAsFactors = FALSE) 
    colnames(df) <- c("prediction","diagnosed","detect")

    detected <- df[df$diagnosed=="M",]
    sumdetect <- sum(detected$detect==TRUE)
    lendetect <- dim(detected)[1]
    sumdetect # trf[2,2]
    sumdetect / lendetect
}

get_selectivity <- function(predictions, diagnosis){
    detection <- predictions == diagnosis
    df <- data.frame(predictions, diagnosis, detection, stringsAsFactors = FALSE) 
    colnames(df) <- c("prediction", "diagnosed", "detect")

    cleared <- df[df$diagnosed=="B",]
    sumcleared <- sum(cleared$detect==TRUE)
    lencleared <- dim(cleared)[1]
    sumcleared # trf[1,1]
    sumcleared / lencleared
}

```

<br><br>

#### Correlation with Diagnosis

The top row of the heat map below in Figure-1 indicates the degree of correlation with the diagnosis for variables with correlation coefficients greater than 0.6 (r > 0.6). The dark square at the upper right corner of the map indicates the trivial correlation of the diagnosis with itself as do the other squares along the same diagonal. The squares across the top row indicate the correlation of the other variables with the diagnosis. The next darkest shaded squares in the top row are for the mean perimeter(perimeter_mean), worst radius (radius_worst), and worst perimeter (perimeter_worst).  

```{r heatmapcalc, echo=FALSE, results='hide', message=F}

smaller <- traina[,c(-1, #-2,-3,-4,-5,-6,-7,-8,-9,-10,
                     #-11,-12,-13,-14,-15,-16,-17,-18,-19,-20,
                     #-21,-22,-23,-24,-25,-26,-27,
                     -33,-34)]
alphas <- as.character(smaller$diagnosis)
alphas[alphas=="B"] <- 0
alphas[alphas=="M"] <- 1
alphas <- as.numeric(alphas)

smaller[,"diagnosis"] <- alphas
cols <- dim(smaller)[2]
colvec <- c(1:cols)
corrs <- unlist(lapply(colvec, function(x){cor(alphas, smaller[,colvec[x]])[[1]][1]}))
helpfulcols <- corrs > 0.6 
helpful <- smaller[,helpfulcols]

cols2 <- dim(helpful)[2]
colvec2 <- c(1:cols2)

corrmatrix1 <- cor(helpful)
#corrs2 <- findCorrelation(corrmatrix1, cutoff = .997, verbose = TRUE)
#remaining <- colvec2[-corrs2]

#reduced <- helpful[,remaining]
#corrmatrix2 <- cor(reduced)

expanded <- expand.grid(row = colnames(corrmatrix1), col = colnames(corrmatrix1))
#expanded <- expand.grid(row = colnames(corrmatrix2), col = colnames(corrmatrix2))
nameslist <- colnames(helpful)

#vec <- as.vector(corrmatrix2)
vec1 <- as.vector(corrmatrix1)
expanded[,"correlation"] <- vec1
#expanded[,"correlation"] <- vec
```


```{r heatmap, echo=FALSE, results='asis', fig.width=10, fig.height=8.5}

library(RColorBrewer)
#cols <- brewer.pal(11, "RdBu")
rgbpalette <- colorRampPalette(c(rgb(0,0,0.99),rgb(0.99,0.99,0.99),rgb(0.8,0.2,0)),   space = "Lab")
heatmap <- levelplot(correlation ~ row + col, expanded, col.regions=rgbpalette(120), 
          xlab = " ", ylab=list(label=" "), 
          scales=list(x=list(rot=90, cex=1.2, col='black'), y=list(cex=1.2, alternating=1, col='black')),
          main="Correlation of Most Predictive Variables (r > 0.6)")

grid.arrange(textGrob("Figure-1", gp=gpar(fontsize=16, col="darkgreen")),
        heatmap, ncol=1,
        layout_matrix = rbind(c(1), c(2)),
        heights=unit(c(16,210), c("mm", "mm")))

```

#### Correlation between Predictive Variables

If we remove the top horizontal row and the vertical column at the right edge in Figure-1 above, the remainder is a heat map for the correlation between predictive variables. The red area at the lower left indicates high correlation between the mean radius, perimeter, and area. The red area near the midpoint of the bottom edge indicates high correlation between these basic dimension variables and the worst parameters of the same variables. The blue areas above these two groups show a lower correlation between the basic dimensions and the concavity and compactness. 

<br><br>

#### Decomposition in Principal Components

The variables are transformed into orthogonal or statistically independent components using a linear algebra function called "prcomp" in an R package. The three biggest independent components separate benign and malignant cases as shown in Figure-2 below with the third component performing the best. There are many other components and only three are illustrated below. 

```{r pca1, echo=FALSE, results='asis', fig.width=16, fig.height=10}

    typeColor <- (smaller$diagnosis==1) + 1
    typediag <- traina$diagnosis
    helpfulpca <- helpful[,c(-1,-32,-33,-34)]
    princomphelp <- prcomp(helpfulpca)
    
    xhelp <- unname(princomphelp$x[,1])
    yhelp2 <- unname(princomphelp$x[,2])
    yhelp3 <- unname(princomphelp$x[,3])
    
    helpdf2 <- data.frame(xhelp, yhelp2, typediag)
    colnames(helpdf2) <- c("PC1","PC2","Diagnosis")
    
    helpdf3 <- data.frame(xhelp, yhelp3, typediag)
    colnames(helpdf3) <- c("PC1","PC3","Diagnosis")

     plotpca2 <- ggplot(helpdf2, aes(PC1, PC2)) + 
        geom_point(aes(color = Diagnosis), alpha=1, size=3) +  
        scale_color_manual(values=c("turquoise","brown")) +
        coord_cartesian(xlim = c(-2600, 800), ylim = c(-300, 600)) +
        #xlab("PC") +
        #ylab("PC2") +
        scale_x_continuous(minor_breaks = seq(-2600 , 2200, 50), breaks = seq(-2600, 2200, 200)) +
        scale_y_continuous(minor_breaks = seq(-300 , 1100, 10), breaks = seq(-300, 1100, 50)) +
        #geom_abline(slope = m1, intercept = int1, size=1.3, color="#006633") +
        ggtitle(" ") +
        theme(plot.title = element_text(hjust = 0.5, color="#993333", size=18, face="bold.italic"),
            axis.title.x = element_text(color="#993333", size=16),
            axis.title.y = element_text(color="#993333", size=16),
            panel.grid.minor =   element_line(colour = "white",size=0.6),
            panel.grid.major =   element_line(colour = "light blue",size=0.8),
            panel.background = element_rect(fill = "#EFE5E5", colour = "brown",
            size = 0.5, linetype = "solid"),
            plot.background = element_rect(fill = "#E0DADA"))
    
    plotpca3 <- ggplot(helpdf3, aes(PC1, PC3)) + 
        geom_point(aes(color = Diagnosis), alpha=1, size=3) +  
        scale_color_manual(values=c("turquoise","brown")) +
        coord_cartesian(xlim = c(-2600, 800), ylim = c(-20, 20)) +
        #xlab("PC") +
        #ylab("PC2") +
        scale_x_continuous(minor_breaks = seq(-2600 , 2200, 50), breaks = seq(-2600, 2200, 200)) +
        scale_y_continuous(minor_breaks = seq(-20 , 30, 1), breaks = seq(-20, 30, 5)) +
        #geom_abline(slope = m1, intercept = int1, size=1.3, color="#006633") +
        ggtitle(" ") +
        theme(plot.title = element_text(hjust = 0.5, color="#993333", size=18, face="bold.italic"),
            axis.title.x = element_text(color="#993333", size=16),
            axis.title.y = element_text(color="#993333", size=16),
            panel.grid.minor =   element_line(colour = "white",size=0.6),
            panel.grid.major =   element_line(colour = "light blue",size=0.8),
            panel.background = element_rect(fill = "#EFE5E5", colour = "brown",
            size = 0.5, linetype = "solid"),
            plot.background = element_rect(fill = "#E0DADA"))

    grid.arrange(textGrob("Figure-2A: Second vs. First PCA Component",gp=gpar(fontsize=16, col="darkgreen")),
                 textGrob("Figure-2B: Third vs. First PCA Component",gp=gpar(fontsize=16, col="darkgreen")),  plotpca2,  plotpca3, ncol=2,
             layout_matrix = rbind(c(1,2), 
                                   c(3,4),
                                   c(3,4)),
             heights=unit(c(10,85,85), c("mm", "mm","mm")))


```


<br><br>

### Different Types of Serial Numbers

There are six different serial numbers lengths from four to nine digits. Since the characteristics of observation records with lengths of 4 and 9 digits seem to vary a little from the other records, group numbers were assigned that indicate the length of the serial number which varies from four to nine digits. Groups 4 and 9 were deleted from this study because Group-49 has a steeper fitted line than Group-8 in Figure-3A below. Group-49 consists of Group-4 and Group-9 combined into a single group.

Based on the what Figure-3A indicates for Groups-4 and Group-9, we study the remaining groups in Figure-3B where Groups 7 and 8 seems to be a representative subgroup for the remaining groups. We build and test our prediction algorithm using the subset of observations that have 7 and 8 digit serial numbers, defining this subset as Group-78 which consists of a subgroup of 81 rows of observation records. Analysis of the other Groups 5 and 6 could be made also as they different a little from Group-78. The algorithm training tryout group consisted of 49 randomly selected records from Group-8 with 21 reserved for final validation testing. 

Since there are 357 benign and 212 malignant cases in the entire dataset. Group-78 consists of 38 benign and 43 malignant cases which is a higher rate of malignant cancers and requires future study. 


```{r table1, echo=FALSE, results='asis', fig.width=4, fig.height=2}

Serial_Number_Length <- as.factor(c("(digits)", "4", "5", "6", "7", "8", "9"))
Quantity <- c(" ",dim(train4)[1], dim(train5)[1], dim(train6)[1], dim(train7)[1], dim(train8)[1], dim(train9)[1])
df1 <- data.frame(Serial_Number_Length, Quantity)
group_size <- xtable(df1)
print(xtable(group_size, caption = "Table-1: Serial Number Groups", align = "ccc", size="14pt"),  caption.placement ='top',
      include.rownames=FALSE, type = "html", latex.environments = "center")  

```

<br><br>

Plot-2 shows the data is more uniform with Group-49 deleted. Notice that Group-8 in brown seems to represent typical members with this change. For the remainder of this study, we focus on Group-8. 

<br><br>

### Exploration of Group-78

#### Separating Benign (B) and Malignant (M) Clusters

```{r plotgroup1, echo=FALSE, fig.width=14, fig.height=7}

m49 <- get_concave_points_fit(train49)[2]
b49 <- get_concave_points_fit(train49)[1]
m8 <- get_concave_points_fit(train8)[2]
b8 <- get_concave_points_fit(train8)[1]
plot1 <- get_concave_points_2Group_plot(train498,m49,b49,m8,b8)

```

```{r plotgroup2, echo=FALSE, fig.width=14, fig.height=7}

linefit5678 <- lm(train5678$concave_points_worst ~ train5678$compactness_mean)
m5678 <- linefit5678$coefficients[2]
b5678 <- linefit5678$coefficients[1]
plot2 <- get_concave_points_plot(train5678,m5678,b5678,m8,b8,"Remaining Data without Group-49")

grid.arrange(textGrob("Figure-3A",gp=gpar(fontsize=16, col="darkgreen")),textGrob("Figure-3B",gp=gpar(fontsize=16, col="darkgreen")), plot1, plot2, ncol=2,
             layout_matrix = rbind(c(1,2), 
                                   c(3,4),
                                   c(3,4)),
             heights=unit(c(10,85,85), c("mm", "mm","mm")))


```

<br><br>

#### Method-1: k-means

A plot of the worse concave points versus mean compactness appears to separate the malignant and benign data points into two loose clusters in Plot-4A below. The center of the benign (B) and malignant (M) clusters was determined using the kmeans function and are indicated by crosses in the figure. To help predict the diagnosis, the assigned cluster group is added to the raw data frame. The fitted lines for the clusters are distinct from one another with the line through the malignant points higher in the plot. This indicates greater worst concave points dimensions for a given degree of compactness for malignant nuclei.

```{r plotgroup3a, echo=FALSE}

kmeanstrain <- get_kmeans(train78$compactness_mean, train78$concave_points_worst)
kmeanstest <- get_kmeans(test_data$compactness_mean, test_data$concave_points_worst)

mm8 <- get_line_parameters(train78,"M")[1]
bm8 <- get_line_parameters(train78,"M")[2]
mb8 <- get_line_parameters(train78,"B")[1]
bb8 <- get_line_parameters(train78,"B")[2]

plot3A <- get_concave_points_kmeans_plot(train78,mm8,bm8,mb8,bb8,kmeanstrain,"Worst Concave Points")


```

<br><br>

#### Method-2: Adjusting Detection Power versus False Alarms (ROC Curve) - Using Improvised Rules

A plot of worst concavity versus mean perimeter in Figure-4B below also divides the data into two groups but this time the nature of the separation does not lend itself to k-means defined clusters because the range of mean perimeters for malignant nuclei also covers part of the range for benign nuclei. But we can devise some improvised formulas that isolate the benign tumors at the lower left corner of the plot as follows.

###### Rule-1: The tumors are all benign if the worst concavity is below 100 and below the imposed divison line.
###### Rule-2: Otherwise assume tumors are malignant. 

Rule-1 corresponds to only accepting nuclei as benign for blue points in the lower left of Plot-4B. Adding our imposed rule to the data forces the formula to skew prediction probabilities in favor of detecting malignant cases. 

<br><br>

```{r plotgroup3b, echo=FALSE, fig.width=14, fig.height=7}

# mm8b <- get_concavity_fit(train78,"M")[2]
# bm8b <- get_concavity_fit(train78,"M")[1]
# mb8b <- get_concavity_fit(train78,"B")[2]
# bb8b <- get_concavity_fit(train78,"B")[1]

plot3B <- get_concavity_worst_plot(train78,-0.018,1.85,"Worst Concavity")
bg = "wheat1"
grid.arrange(textGrob("Figure-4A: K-Means Clusters",gp=gpar(fontsize=16, col="darkgreen")), textGrob("Figure-4B: Adjustment with Improvised Rule",gp=gpar(fontsize=16, col="darkgreen")), plot3A, plot3B, ncol=2,
             layout_matrix = rbind(c(1,2), 
                                   c(3,4),
                                   c(3,4)),
             heights=unit(c(10,84,84), c("mm", "mm","mm")))

```


Rule-2 rule is defined by the dotted brown line in Plot-3B. It helps predict if tumors are benign or malignant but it also provides a way to adjust the automated predictions provided by the carrot package in order to optimize the power of detecting malignant nuclei at the cost of producing more false alarms. There is an inherent tradeoff between detection power and number of false alarms. Our goal is to optimize the overall cost of missed detections and false alarms instead of an overall accuracy figure.

<br><br>


```{r plotgroup4a, echo=FALSE, fig.width=14, fig.height=7}

kmeanstrain <- get_kmeans(train78$perimeter_mean, train78$concave_points_worst)
kmeanstest <- get_kmeans(test_data$perimeter_mean, test_data$concave_points_worst)

mm8 <- get_line_parameters(train78,"M")[1]
bm8 <- get_line_parameters(train78,"M")[2]
mb8 <- get_line_parameters(train78,"B")[1]
bb8 <- get_line_parameters(train78,"B")[2]

plot4a <- get_concave_points_plot2(train78,mm8,bm8,mb8,bb8,kmeanstrain,"Worst Concave Points")


```

```{r plotgroup4b, echo=FALSE, fig.width=14, fig.height=7}

# mm8 <- get_perimeter_line(train78,"M")[1]
# bm8 <- get_perimeter_line(train78,"M")[2]
mb8 <- get_perimeter_line(train78,"B")[1]
bb8 <- get_perimeter_line(train78,"B")[2]

plot4b <- get_perimeter_plot(train78,mb8,bb8,"Worst Perimeter")
bg = "wheat1"
grid.arrange(textGrob("Figure-5A: K-Means Clusters",gp=gpar(fontsize=16, col="darkgreen")), textGrob("Figure-5B: Regression Adjustment",gp=gpar(fontsize=16, col="darkgreen")), plot4a, plot4b, ncol=2,
             layout_matrix = rbind(c(1,2), 
                                   c(3,4),
                                   c(3,4)),
             heights=unit(c(10,84,84), c("mm", "mm","mm")))

```



<br><br>

#### Regression Analysis Help

If the data for benign and malignant cases appear mixed together or cover the same range, we might still be able to separate them using regression. In Plot-6A, the worst perimeter measurements for the two groups overlap; but after regression against the mean radius is applied in Plot-6B, benign and malignant cases separate into distinct groups.  

```{r misc_plots, echo=FALSE, fig.width=14, fig.height=7}

plot5 <- ggplot(traina, aes(diagnosis, perimeter_worst)) + 
    geom_point(aes(color = diagnosis),alpha=0.2, size=3) + #hjust = -0.4, vjust = 1.5
    scale_color_manual(values=c("turquoise","brown")) +
    xlab("Benign (B) or Malignant (M) Diagnosis") +
    ggtitle("Worst Perimeter") +
    ylab("Worst Perimeter (µm)") +
    scale_y_continuous(minor_breaks = seq(0 , 240, 10), breaks = seq(0, 240, 20)) +
    coord_cartesian(ylim = c(60, 220)) +
    theme(plot.title = element_text(hjust = 0.5, color="#993333", size=18, face="bold.italic"),
         axis.title.x = element_text(color="#993333", size=16),
         axis.title.y = element_text(color="#993333", size=16),
         panel.grid.minor =   element_line(colour = "white",size=0.6),
         panel.grid.major =   element_line(colour = "light blue",size=0.8),
         panel.background = element_rect(fill = "#EFE5E5", colour = "brown",
             size = 0.5, linetype = "solid"),
         plot.background = element_rect(fill = "#E0DADA"))

plot6 <- ggplot(traina, aes(radius_mean, perimeter_worst)) + 
    geom_point(aes(color = diagnosis),alpha=0.5, size=3) +
    scale_color_manual(values=c("turquoise","brown")) +
    xlab("Mean Radius (µm)") +
    scale_x_continuous(minor_breaks = seq(0 , 40, 2), breaks = seq(0, 40, 4)) +
    scale_y_continuous(minor_breaks = seq(0 , 240, 10), breaks = seq(0, 240, 20)) +
    coord_cartesian(xlim = c(8, 24), ylim = c(60, 220)) +
    geom_abline(slope = 5, intercept = 30, size=1.3, color="darkgreen") +
    ylab("Worst Perimeter (µm)") +
    ggtitle("Worst Perimeter vs. Mean Radius") +
    theme(plot.title = element_text(hjust = 0.5,
                        color="#993333", size=18, face="bold.italic"),
         axis.title.x = element_text(color="#993333", size=16),
         axis.title.y = element_text(color="#993333", size=16),
         panel.grid.minor =   element_line(colour = "white",size=0.6),
         panel.grid.major =   element_line(colour = "light blue",size=0.8),
         panel.background = element_rect(fill = "#EFE5E5", colour = "brown",
             size = 0.5, linetype = "solid"),
         plot.background = element_rect(fill = "#E0DADA"))

grid.arrange(textGrob("Figure-6A: Before Applying Regression",gp=gpar(fontsize=16, col="darkgreen")),textGrob("Figure-6B: After Regression",gp=gpar(fontsize=16, col="darkgreen")), plot5, plot6, ncol=2,
             layout_matrix = rbind(c(1,2), 
                                   c(3,4),
                                   c(3,4)),
             heights=unit(c(10,84,84), c("mm", "mm","mm")))

```

<br><br>

##   Prediction Model

The best prediction models include independent uncorrelated variables but these models usually need to be compared with simpler types that are easier to interpret. An automated way to include only uncorrelated variables in a prediction model is possible with linear algebra and can be implemented with principal components analysis (PCA).  

Two prediction models were contructed. The first method is a random forest method because it usually works well in most cases. The second model was based on pre-processing with principal components analysis followed by random forest model fitting using the uncorrelated principal components. Confusion matrix for these methods are presented at the end of the report in Tables 3 and 4 respectively. 

For the first method with random forest alone, the variables are chosen based on careful study. Variables from both blue and red squares in the heat map at Figure-1 were selected because the degree of correlation is less between these areas. Many correlated variables were were eliminated to simplify the model and reduce the noise level. For example, the radius, perimeter, and area are highly correlated and only one of these is used for any given comparison. The worst perimeter, worst concavity, and worst concave points were selected for the model because they are highly correlated with the diagnosis as can be determined by the top row of the heat map. They were plotted against the mean compactness and mean perimeter as regressor variables to gain further insight.

<br><br>

## Performance

<br><br>


```{r fitmodels, echo = FALSE, message = FALSE}
#, warning = F}

    trainb <- get_calculated_data(traina, kmeanstrain, m=-0.018, b=1.85, cutoff=0.05)[,c(-1,-33,-34)]
    test_datab <- get_calculated_data(test_data, kmeanstest, m=-0.018, b=1.85, cutoff=0.05)[,c(-1,-33,-34)]
    
    fitControl <- trainControl(## 3-fold CV
            method = "repeatedcv", number = 3, repeats = 3,
            summaryFunction = twoClassSummary, 
            classProbs = T,
            savePredictions = T)
    
    set.seed(32323)
    rffit2 <- train(diagnosis ~ perimeter_mean + compactness_mean +
                                concave_points_worst + concavity_worst +
                                concave_points_cluster + concavity_cluster,
            method="rf", trControl = fitControl, #  verbose = FALSE,
            metric = "ROC", data=trainb)
    
    Actual_Diagnosis <- test_datab[,31]
    test_predictions <- predict(rffit2, newdata=test_datab[,c(-31)])

##  Principal Components Model  
    
    typeColor <- (trainb$diagnosis=="M") + 1
   
    pp_data <- preProcess(trainb[,c(-31)],method="pca",pcaComp=30)
    trainPC <- predict(pp_data,trainb[,c(-31)]) 
    
    trainPC[,"diagnosis"] <- trainb[,31]
    modelFit <- train(diagnosis ~ ., method="rf",data=trainPC)
    
    pp_test <- preProcess(test_datab[,c(-31)], method="pca", pcaComp=30)
    test_pc <- predict(pp_data, test_datab[,c(-31)])
    pca_test_predictions <- predict(modelFit, test_pc)
    
```

<br><br>

###  Training Test Results with Random Forest Method 

```{r confusion_matrix1, echo=FALSE, results='asis', fig.width=4, fig.height=2}

Testing_Accuracy <- test_predictions==test_datab[,31]
PCA_Testing_Accuracy <- pca_test_predictions==test_datab[,31]

predictions <- data.frame(test_predictions,Actual_Diagnosis, Testing_Accuracy)
pca_predictions <- data.frame(pca_test_predictions,Actual_Diagnosis, PCA_Testing_Accuracy)
#colnames(predictions) <- c("Prediction","Actual_Diagnosis", "Accuracy")

xt3 <- xtable(predictions)

print(xtable(xt3, caption = "Table-2: Results for Test Set", align = "cccc", size="14pt"),
      caption.placement ='top', include.rownames=FALSE, type = "html")  

```

<br><br>

### Detection Power versus False Alarms (ROC Curve) for Random Forest Model without Principal Components
This is an inherent tradeoff between detection power and the number of false alarms. This curve was generated using R Tools carrot, pROC, plotROC using the train function output for the prediction model. The "pred" list item of the output contains the data frame for the curve. 

<br><br>

#####    Sensitivity: The probability of a positive test result if the disease is present
#####    Specificity: The probability of negative test if the disease is not present
#####    Probability of False Alarm  =  1 - Specificity

<br><br>


```{r roc_curve, echo=FALSE, results='asis', fig.width=7, fig.height=7}

# Select a parameter setting
selectedIndices <- rffit2$pred$mtry == 2

ggplot(rffit2$pred[selectedIndices, ], aes(m = M, d = factor(obs, levels = c("B", "M")))) + 
    geom_roc(hjust = -0.4, vjust = 1.5) +
    coord_equal() +
    xlab("Probability of False Alarm (1 - Specificity)") +
    ylab("Probability of Detecting Malignant Tumor") +
    ggtitle("Figure-7: ROC Curve for Prediction Model") +
    scale_x_continuous(minor_breaks = seq(0 , 1, 0.05), breaks = seq(0, 1, 0.1)) +
    scale_y_continuous(minor_breaks = seq(0 , 1, 0.05), breaks = seq(0, 1, 0.1)) +
    coord_cartesian(ylim = c(0, 1)) +
    theme(plot.title = element_text(hjust = 0.5, color="#993333", size=18, face="bold.italic"),
         axis.title.x = element_text(color="#993333", size=16),
         axis.title.y = element_text(color="#993333", size=16),
         panel.grid.minor =   element_line(colour = "white",size=0.6),
         panel.grid.major =   element_line(colour = "light blue",size=0.8),
         panel.background = element_rect(fill = "#EFE5E5", colour = "brown",
             size = 0.5, linetype = "solid"),
         plot.background = element_rect(fill = "#E0DADA"))
    
``` 

<br><br> 

Rather than overall accuracy, the probabilities and costs of detection and false alarms are important in the case of radar systems and in other problems such as cancer detection. In the case of a defensive radar system, selectivity determines the probability shooting down your own plane and injuring a friendly pilot. It might be less than optimal to focus on an overall accuracy that mixes sensitivity and selectivity into a single parameter when the costs of failing to detect a condition (sensitivity) and false alerts (the complement of sensitivity) are separately important with their own costs. 

To help control our model, the cluster identification for k-means for worst concave points as shown in Plot-4A and worst concavity according to the dotted lines in Plot-4B were added to the data frame. Adding extra parameters should help adjust the ROC curve by raising the detection power for malignant tumors in order to optimize the power of detecting existing malignant tumors at the cost of producing more false alarms.

<br><br>

### Simplified ROC Curve for Worst Concave Points Test Set Results 
This characteristic curve is based only based on the worst concave points of the test. 

```{r trainingroc, echo=FALSE, results='asis'}

pm <- test_datab$perimeter_mean

bin <- cut(test_datab$concave_points_worst, breaks = c(0,seq(0.16,0.28, by = 0.02)), labels = 1:7)
dft <- data.frame(test_predictions, Actual_Diagnosis, Testing_Accuracy, pm, bin)
dft[,"concave_points_worst"] <- test_datab$concave_points_worst
dft <- dft[order(dft[,"concave_points_worst"]),]

    total <- 0        # Number of patients in bin
    truepos <- 0    # Number of true positives
    falsepos <- 0   # Number of false positives
    totpos <- 0     # The number of positives
    totneg <- 0     # The number of negatives
    nr <- 0
    sens <- c()
    omspec <- c()

for(i in unique(dft$bin)){
    tdat <- dft[dft$bin == i,]
    nr <- dim(tdat)[1] + nr
    tdiag <- tdat$Actual_Diagnosis
    tbin <- tdat$bin
    tpred <- dft$test_predictions[dft$bin == i]
    tab <- as.matrix(table(tdiag, tpred)) #test_predictions
    
    tot <- dim(tdat)[1]                             # Number of patients in bin
    truep <- tab[2,2]   # Number of true positives
    falsep <- tab[2,1]  # Number of false negatives
    totp <- sum(tab[2,])                          # The total number of positives (one number)
    totn <- sum(tab[1,])                          # The total number of negatives (one number)
    
    truepos <- c(truepos, truep)
    falsepos <- c(falsepos, falsep)
    totpos <- c(totpos, totp)
    totneg <- c(totneg, totn)
 
}
  
ss <- sum(totpos)      
if (ss > 0 ) {                         # Sensitivity (fraction true positives)
        
        csens <- cumsum(truepos/ss)
} 
 so <- sum(totneg)
if (so > 0)  {                       # specificity (false positives)                
       
        comspec <- cumsum(falsepos)/so
}        

csens <- c(0, csens, 1)
comspec <- c(0, comspec, 1)           # Numbers when we classify all as normal

rocplot <- data.frame(comspec,csens)

plot(comspec, csens, type="b", xlim=c(0,1), ylim=c(0,1), lwd=2,
     xlab="Probability of False Alarm (1 - Specificity)", ylab="Sensitivity", main="Figure-8: Simple ROC Curve Based on Test Set")  

    grid()
    abline(0,1, col="red", lty=2)
    
  
```

<br><br> 

### Confusion Matrix for Random Forest Prediction

The table below describes the results for the test set.

```{r confusion_matrix, echo=FALSE, results='asis', fig.width=4, fig.height=2}

cm <- confusionMatrix( data = test_predictions,     test_data$diagnosis)
cm2 <- confusionMatrix(data = pca_test_predictions, test_data$diagnosis)

benign_test <- Actual_Diagnosis=="B"
mal_test <- Actual_Diagnosis=="M"

negative_count <- sum(predictions[Actual_Diagnosis=="B" & test_predictions=="B","Testing_Accuracy"])
positive_count <- sum(predictions[Actual_Diagnosis=="M" & test_predictions=="M","Testing_Accuracy"])
pca_negative_count <- sum(pca_predictions[Actual_Diagnosis=="B" & pca_test_predictions=="B","Testing_Accuracy"])
pca_positive_count <- sum(pca_predictions[Actual_Diagnosis=="M" & pca_test_predictions=="M","Testing_Accuracy"])

real_benign_count <- sum(Actual_Diagnosis=="B")
real_malignant_count <- sum(Actual_Diagnosis=="M")

fa <- negative_count / real_benign_count
pd <- positive_count / real_malignant_count
fa_pca <- pca_negative_count / real_benign_count
pd_pca <- pca_positive_count / real_malignant_count

########################################################################

Parameter <- c("Detection Power","False Alarm")
Test <- c("Sensitivity", "1 - Specificity")
Diagnosis <- c("Malignant","Benign")
Counts <- c(real_malignant_count,real_benign_count)

Correct_Detections <- c(positive_count, negative_count)
Accuracy <- c(cm$byClass[2], cm$byClass[1])

dfconfus1 <- data.frame(Parameter, Test, Diagnosis,Counts,Correct_Detections,Accuracy)
xt4 <- xtable(dfconfus1)

print(xtable(xt4, caption = "Table-3: Prediction Accuracy for Basic Random Forest Model", align = "ccccccc", size="14pt"),
      caption.placement ='top', include.rownames=FALSE, type = "html")


```

<br><br>

### Confusion Matrix for Principal Components Analysis

The table below describes the results for the test set.

```{r pca_confusion_matrix, echo=FALSE, results='asis', fig.width=4, fig.height=2}

Parameter <- c("Detection Power","False Alarm")
Test <- c("Sensitivity", "1 - Specificity")
Diagnosis <- c("Malignant","Benign")
Counts <- c(real_malignant_count,real_benign_count)

Correct_Detections <- c(pca_positive_count, pca_negative_count)
Accuracy <- c(cm2$byClass[2], cm2$byClass[1])

pcaconfus <- data.frame(Parameter, Test, Diagnosis,Counts,Correct_Detections,Accuracy)
xt5 <- xtable(dfconfus1)

print(xtable(xt5, caption = "Table-4: PCA Prediction Accuracy", align = "ccccccc", size="14pt"),
      caption.placement ='top', include.rownames=FALSE, type = "html")

```

<br><br> 

Code for this reproducible report is available at https://github.com/marcelMerchat/breastCancer_WI.

<br><br>

#### THE END

